{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3419588,"sourceType":"datasetVersion","datasetId":2061039}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nimport sys\nimport random\nimport json\nimport warnings\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Reinforcement Learning\nimport gym\nfrom gym import spaces\nfrom stable_baselines3 import PPO, A2C, DQN\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.callbacks import BaseCallback\n\n# Model interpretability\ntry:\n    import shap\n    SHAP_AVAILABLE = True\nexcept ImportError:\n    SHAP_AVAILABLE = False\n    print(\"‚ö†Ô∏è  SHAP not available - interpretability features disabled\")\n","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppress warnings for cleaner output\nwarnings.filterwarnings(\"ignore\")\n\n# Set random seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\n# Matplotlib styling\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n# Configuration dataclass\n@dataclass\nclass Config:\n    \"\"\"IMPROVED configuration parameters\"\"\"\n    seed: int = 42\n    train_split: float = 0.70\n    val_split: float = 0.15\n    test_split: float = 0.15\n    min_episode_length: int = 3\n    training_timesteps: int = 150000  # INCREASED for better learning\n    eval_episodes: int = 20\n    history_window: int = 5\n    \n    # NEW: Training hyperparameters\n    learning_rate: float = 0.0003\n    gamma: float = 0.99\n    batch_size: int = 64\n    \nconfig = Config()\n\nprint(\"‚úÖ All libraries imported successfully!\")\nprint(f\"üìä Configuration: Train={config.train_split}, Val={config.val_split}, Test={config.test_split}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 2: LAB ITEM METADATA DEFINITION\n# PURPOSE: Define standardized metadata for key MIMIC-III lab tests\n# ==============================================================================\n\nLAB_ITEM_INFO = {\n    50931: {  # Glucose\n        'name': 'Glucose',\n        'normal_range': (70.0, 100.0),\n        'unit': 'mg/dL',\n        'critical': True,\n        'description': 'Blood sugar level'\n    },\n    50912: {  # Creatinine\n        'name': 'Creatinine',\n        'normal_range': (0.6, 1.2),\n        'unit': 'mg/dL',\n        'critical': True,\n        'description': 'Kidney function marker'\n    },\n    50902: {  # Chloride\n        'name': 'Chloride',\n        'normal_range': (98.0, 106.0),\n        'unit': 'mEq/L',\n        'critical': False,\n        'description': 'Electrolyte balance'\n    },\n    50882: {  # Bicarbonate\n        'name': 'Bicarbonate',\n        'normal_range': (22.0, 29.0),\n        'unit': 'mEq/L',\n        'critical': False,\n        'description': 'Acid-base balance'\n    },\n    50971: {  # Potassium\n        'name': 'Potassium',\n        'normal_range': (3.5, 5.0),\n        'unit': 'mEq/L',\n        'critical': True,\n        'description': 'Critical electrolyte affecting heart rhythm'\n    },\n    50983: {  # Sodium\n        'name': 'Sodium',\n        'normal_range': (136.0, 145.0),\n        'unit': 'mEq/L',\n        'critical': True,\n        'description': 'Primary extracellular electrolyte'\n    },\n    51006: {  # Urea Nitrogen (BUN)\n        'name': 'Urea Nitrogen',\n        'normal_range': (7.0, 20.0),\n        'unit': 'mg/dL',\n        'critical': False,\n        'description': 'Protein metabolism marker; indicates kidney function'\n    },\n    50868: {  # Anion Gap\n        'name': 'Anion Gap',\n        'normal_range': (8.0, 16.0),\n        'unit': 'mEq/L',\n        'critical': False,\n        'description': 'Calculated marker for acid-base balance'\n    },\n    51265: {  # Platelet Count\n        'name': 'Platelet Count',\n        'normal_range': (150.0, 400.0),\n        'unit': 'K/uL',\n        'critical': False,\n        'description': 'Clotting ability indicator'\n    },\n    51221: {  # Hematocrit\n        'name': 'Hematocrit',\n        'normal_range': (36.0, 46.0),\n        'unit': '%',\n        'critical': False,\n        'description': 'Proportion of red blood cells in blood'\n    },\n    51301: {  # WBC\n        'name': 'WBC',\n        'normal_range': (4.5, 11.0),\n        'unit': 'K/uL',\n        'critical': True,\n        'description': 'White blood cell count ‚Äî infection or inflammation marker'\n    },\n    51222: {  # Hemoglobin\n        'name': 'Hemoglobin',\n        'normal_range': (13.5, 17.5),\n        'unit': 'g/dL',\n        'critical': True,\n        'description': 'Oxygen-carrying capacity of blood'\n    }\n}\n\nprint(f\"‚úÖ Loaded metadata for {len(LAB_ITEM_INFO)} lab tests\")\nprint(\"\\nüìã Lab Tests:\")\nfor itemid, info in LAB_ITEM_INFO.items():\n    print(f\"  {itemid}: {info['name']} \"\n          f\"(Normal: {info['normal_range'][0]}‚Äì{info['normal_range'][1]} {info['unit']}) \"\n          f\"| Critical: {info['critical']}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SECTION 3: DATA LOADING FROM MIMIC-III\n# PURPOSE: Load and perform initial filtering of lab events data\n# ===============================================================================\n\n# Path to MIMIC-III data (adjust based on your setup)\nDATA_PATH = \"/kaggle/input/mimiciii/mimic-iii-clinical-database-demo-1.4\"\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìÇ LOADING MIMIC-III DATA\")\nprint(\"=\"*80)\n\n# Load lab events\nprint(\"Loading LABEVENTS.csv...\")\nlabevents = pd.read_csv(f\"{DATA_PATH}/LABEVENTS.csv\")\nprint(f\"‚úÖ Loaded {len(labevents):,} raw lab measurements\")\n\n# Select relevant columns\nlabevents = labevents[['subject_id', 'hadm_id', 'itemid', 'charttime', 'valuenum', 'valueuom']]\n\n# Basic cleaning\nprint(\"\\nüßπ Cleaning data...\")\ninitial_count = len(labevents)\nlabevents = labevents.dropna(subset=['valuenum'])\nprint(f\"  ‚Ä¢ Removed {initial_count - len(labevents):,} rows with missing values\")\n\nlabevents['charttime'] = pd.to_datetime(labevents['charttime'], errors='coerce')\nlabevents = labevents.dropna(subset=['charttime'])\n\n# Filter to top 10 most common lab items\nprint(\"\\nüî¨ Filtering to top 10 lab items...\")\ntop_items = labevents['itemid'].value_counts().head(10).index.tolist()\nlabevents_filtered = labevents[labevents['itemid'].isin(top_items)]\n\nprint(f\"‚úÖ Filtered to {len(labevents_filtered):,} measurements\")\nprint(f\"üìä Top 10 Lab Items: {top_items}\")\n\n# Display summary statistics\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä DATA SUMMARY\")\nprint(\"=\"*80)\nprint(labevents_filtered.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SECTION 4: ADVANCED DATA PREPROCESSING\n# PURPOSE: Transform raw data into structured episodes for RL\n# CRITICAL FIX: Create multi-dimensional states (NOT averaging different labs!)\n# ===============================================================================\n\nclass DataPreprocessor:\n    \"\"\"\n    Advanced data preprocessing for medical RL\n    \n    KEY IMPROVEMENT: Maintains separate values for each lab test type\n    (Original version incorrectly averaged glucose, WBC, creatinine together!)\n    \"\"\"\n    \n    def __init__(self, lab_item_info: Dict, config: Config):\n        self.lab_item_info = lab_item_info\n        self.config = config\n        \n    def create_episodes(self, labevents_df: pd.DataFrame) -> List[Dict]:\n        \"\"\"\n        IMPROVED: Better handling of missing data and outliers\n        \"\"\"\n        \n        episodes = []\n        grouped = labevents_df.groupby(['subject_id', 'hadm_id'])\n        \n        print(f\"\\nüîÑ Processing {len(grouped)} patient admissions...\")\n        \n        for (subject_id, hadm_id), group in grouped:\n            # Pivot table\n            pivot = group.pivot_table(\n                index='charttime',\n                columns='itemid',\n                values='valuenum',\n                aggfunc='mean'\n            )\n            \n            # Sort by time\n            pivot = pivot.sort_index()\n            \n            # IMPROVED: Remove outliers before forward-fill\n            for col in pivot.columns:\n                if col in self.lab_item_info:\n                    normal_range = self.lab_item_info[col]['normal_range']\n                    # Remove values > 10x normal range (likely errors)\n                    upper_limit = normal_range[1] * 10\n                    lower_limit = max(0, normal_range[0] * 0.1)\n                    pivot[col] = pivot[col].clip(lower=lower_limit, upper=upper_limit)\n            \n            # Forward-fill with limit\n            pivot = pivot.fillna(method='ffill', limit=3)  # Max 3 consecutive fills\n            \n            # Backward-fill remaining\n            pivot = pivot.fillna(method='bfill', limit=1)\n            \n            # Drop rows still with NaN\n            pivot = pivot.dropna()\n            \n            # IMPROVED: More lenient length requirement\n            if len(pivot) >= self.config.min_episode_length:\n                episode = {\n                    'subject_id': int(subject_id),\n                    'hadm_id': int(hadm_id),\n                    'timestamps': pivot.index.tolist(),\n                    'lab_values': pivot.values.astype(np.float32),\n                    'lab_items': pivot.columns.tolist(),\n                    'length': len(pivot),\n                    'duration_hours': (pivot.index[-1] - pivot.index[0]).total_seconds() / 3600\n                }\n                \n                # Calculate statistics\n                episode['mean_values'] = pivot.mean().to_dict()\n                episode['std_values'] = pivot.std().to_dict()\n                \n                # Calculate % in normal range\n                normal_count = 0\n                total_count = 0\n                for itemid in pivot.columns:\n                    if itemid in self.lab_item_info:\n                        normal_range = self.lab_item_info[itemid]['normal_range']\n                        values = pivot[itemid].values\n                        in_range = ((values >= normal_range[0]) & \n                                  (values <= normal_range[1])).sum()\n                        normal_count += in_range\n                        total_count += len(values)\n                \n                episode['pct_in_normal_range'] = normal_count / total_count if total_count > 0 else 0\n                \n                episodes.append(episode)\n        \n        print(f\"‚úÖ Created {len(episodes)} valid episodes\")\n        return episodes\n    \n    def split_episodes(self, episodes: List[Dict]) -> Tuple[List, List, List]:\n        \"\"\"\n        Split episodes into train/val/test sets\n        \n        Args:\n            episodes: List of episode dictionaries\n            \n        Returns:\n            Tuple of (train_episodes, val_episodes, test_episodes)\n        \"\"\"\n        \n        n = len(episodes)\n        indices = np.arange(n)\n        np.random.shuffle(indices)\n        \n        train_end = int(self.config.train_split * n)\n        val_end = int((self.config.train_split + self.config.val_split) * n)\n        \n        train_eps = [episodes[i] for i in indices[:train_end]]\n        val_eps = [episodes[i] for i in indices[train_end:val_end]]\n        test_eps = [episodes[i] for i in indices[val_end:]]\n        \n        print(f\"\\\\nüìä Split Summary:\")\n        print(f\"  ‚Ä¢ Training: {len(train_eps)} episodes\")\n        print(f\"  ‚Ä¢ Validation: {len(val_eps)} episodes\")\n        print(f\"  ‚Ä¢ Test: {len(test_eps)} episodes\")\n        \n        return train_eps, val_eps, test_eps\n    \n    def generate_statistics(self, episodes: List[Dict]) -> Dict:\n        \"\"\"Generate comprehensive dataset statistics\"\"\"\n        \n        stats = {}\n        \n        # Episode statistics\n        lengths = [ep['length'] for ep in episodes]\n        durations = [ep['duration_hours'] for ep in episodes]\n        normal_pcts = [ep['pct_in_normal_range'] for ep in episodes]\n        \n        stats['episodes'] = {\n            'count': len(episodes),\n            'length_mean': float(np.mean(lengths)),\n            'length_std': float(np.std(lengths)),\n            'length_min': int(np.min(lengths)),\n            'length_max': int(np.max(lengths)),\n            'duration_mean': float(np.mean(durations)),\n            'duration_std': float(np.std(durations)),\n            'normal_pct_mean': float(np.mean(normal_pcts)),\n            'normal_pct_std': float(np.std(normal_pcts))\n        }\n        \n        return stats\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T14:01:11.538516Z","iopub.execute_input":"2025-10-21T14:01:11.538705Z","iopub.status.idle":"2025-10-21T14:01:11.552500Z","shell.execute_reply.started":"2025-10-21T14:01:11.538691Z","shell.execute_reply":"2025-10-21T14:01:11.551816Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# EXECUTE PREPROCESSING\n# ===============================================================================\n\npreprocessor = DataPreprocessor(LAB_ITEM_INFO, config)\n\n# Create episodes\nepisodes = preprocessor.create_episodes(labevents_filtered)\n\n# Split data\ntrain_episodes, val_episodes, test_episodes = preprocessor.split_episodes(episodes)\n\n# Generate statistics\nstats = preprocessor.generate_statistics(episodes)\n\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"üìà DATASET STATISTICS\")\nprint(\"=\"*80)\nprint(f\"Episode count: {stats['episodes']['count']}\")\nprint(f\"Average length: {stats['episodes']['length_mean']:.1f} ¬± {stats['episodes']['length_std']:.1f}\")\nprint(f\"Length range: {stats['episodes']['length_min']} - {stats['episodes']['length_max']}\")\nprint(f\"Average duration: {stats['episodes']['duration_mean']:.1f} ¬± {stats['episodes']['duration_std']:.1f} hours\")\nprint(f\"% in normal range: {stats['episodes']['normal_pct_mean']:.1%} ¬± {stats['episodes']['normal_pct_std']:.1%}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SECTION 5: REINFORCEMENT LEARNING ENVIRONMENT\n# PURPOSE: Define the MDP (Markov Decision Process) for medical treatment\n# KEY IMPROVEMENT: Multi-dimensional state space with temporal features\n# ===============================================================================\n\nclass MedicalTreatmentEnv(gym.Env):\n    \"\"\"\n    Advanced Medical Treatment Environment\n    \n    STATE SPACE (26 dimensions):\n    - Lab values (10): Current measurements for each lab test\n    - Normalized values (10): Z-scores relative to normal ranges\n    - Short-term trends (5): Recent changes in key labs\n    - Time (1): Normalized time in episode\n    \n    ACTION SPACE (4 discrete actions):\n    - 0: No intervention (watchful waiting)\n    - 1: Medication A (e.g., insulin for glucose control)\n    - 2: Medication B (e.g., diuretic for fluid balance)\n    - 3: Medication C (broad-spectrum stabilization)\n    \n    REWARD STRUCTURE:\n    - Clinical reward: Based on # of abnormal labs and severity\n    - Treatment penalty: Small cost for interventions (avoid over-treatment)\n    - Stability reward: Bonus for maintaining stable values\n    \n    TRANSITION DYNAMICS:\n    - Realistic treatment effects (lab-specific responses)\n    - Stochastic outcomes (variability in patient response)\n    - Delayed effects (treatments take time to work)\n    \"\"\"\n    \n    def __init__(self, episodes_list: List[Dict], lab_item_info: Dict):\n        super().__init__()\n        \n        self.episodes_list = episodes_list\n        self.lab_item_info = lab_item_info\n        \n        # State space: [10 labs + 10 normalized + 5 trends + 1 time] = 26 features\n        self.observation_space = spaces.Box(\n            low=-10.0,\n            high=10.0,\n            shape=(26,),\n            dtype=np.float32\n        )\n        \n        # Action space: 4 discrete treatment options\n        self.action_space = spaces.Discrete(4)\n        \n        # Episode state\n        self.current_episode = None\n        self.current_step = 0\n        self.state_history = []\n        self.history_window = 5\n        \n    def reset(self):\n        \"\"\"Reset environment for new episode\"\"\"\n        \n        # Randomly select an episode\n        self.current_episode = random.choice(self.episodes_list)\n        self.current_step = 0\n        self.state_history = []\n        \n        # Get initial observation\n        obs = self._get_observation()\n        return obs\n    \n    def _get_observation(self) -> np.ndarray:\n        \"\"\"\n        Construct rich observation from current state\n        \n        Returns:\n            26-dimensional observation vector\n        \"\"\"\n        \n        # Get current lab values\n        current_labs = self.current_episode['lab_values'][self.current_step]\n        lab_items = self.current_episode['lab_items']\n        \n        # Ensure we have 10 values (pad if necessary)\n        if len(current_labs) < 10:\n            current_labs = np.pad(current_labs, (0, 10 - len(current_labs)), 'constant')\n            lab_items = lab_items + [0] * (10 - len(lab_items))\n        \n        # 1. Raw lab values (first 10)\n        raw_values = current_labs[:10]\n        \n        # 2. Normalized values (z-scores)\n        normalized = np.zeros(10)\n        for i in range(min(10, len(lab_items))):\n            itemid = lab_items[i]\n            if itemid in self.lab_item_info:\n                normal_range = self.lab_item_info[itemid]['normal_range']\n                mean_normal = (normal_range[0] + normal_range[1]) / 2\n                std_normal = (normal_range[1] - normal_range[0]) / 4\n                normalized[i] = (raw_values[i] - mean_normal) / (std_normal + 1e-6)\n            else:\n                normalized[i] = 0.0\n        \n        # 3. Trend indicators (first 5 labs only)\n        trends = np.zeros(5)\n        if len(self.state_history) > 0:\n            prev_labs = self.state_history[-1][:10]\n            for i in range(5):\n                if prev_labs[i] != 0:\n                    trends[i] = (raw_values[i] - prev_labs[i]) / (abs(prev_labs[i]) + 1e-6)\n        \n        # 4. Time feature\n        time_norm = self.current_step / max(len(self.current_episode['lab_values']) - 1, 1)\n        \n        # Combine all features\n        obs = np.concatenate([\n            raw_values,      # 10 features\n            normalized,      # 10 features\n            trends,          # 5 features\n            [time_norm]      # 1 feature\n        ]).astype(np.float32)\n        \n        # Handle edge cases\n        obs = np.nan_to_num(obs, nan=0.0, posinf=10.0, neginf=-10.0)\n        obs = np.clip(obs, -10.0, 10.0)\n        \n        return obs\n    \n    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:\n        \"\"\"\n        Execute action and transition to next state\n        \n        Args:\n            action: Treatment decision (0-3)\n            \n        Returns:\n            Tuple of (observation, reward, done, info)\n        \"\"\"\n        \n        # Get current state\n        current_labs = self.current_episode['lab_values'][self.current_step].copy()\n        lab_items = self.current_episode['lab_items']\n        \n        # Store in history\n        self.state_history.append(current_labs.copy())\n        if len(self.state_history) > self.history_window:\n            self.state_history.pop(0)\n        \n        # Apply treatment effect\n        if action > 0:\n            current_labs = self._apply_treatment(current_labs, action, lab_items)\n        \n        # Calculate reward\n        reward = self._calculate_reward(current_labs, lab_items, action)\n        \n        # Move to next step\n        self.current_step += 1\n        done = self.current_step >= len(self.current_episode['lab_values']) - 1\n        if done:\n                final_abnormal = self._count_abnormal(current_labs, lab_items)\n                \n                # Big bonus for ending with few abnormal labs\n                if final_abnormal == 0:\n                    reward += 20.0  # Perfect outcome!\n                elif final_abnormal == 1:\n                    reward += 15.0  # Excellent\n                elif final_abnormal == 2:\n                    reward += 10.0  # Very good\n                elif final_abnormal == 3:\n                    reward += 5.0   # Good\n                elif final_abnormal == 4:\n                    reward += 2.0   # Acceptable\n                # No bonus for 5+ abnormal\n\n    \n        # Get next observation\n        if not done:\n            obs = self._get_observation()\n        else:\n            obs = np.zeros(26, dtype=np.float32)\n        \n        # Info for logging\n        info = {\n            'abnormal_count': self._count_abnormal(current_labs, lab_items),\n            'episode_length': len(self.current_episode['lab_values'])\n        }\n        \n        return obs, reward, done, info\n    \n    def _apply_treatment(self, labs: np.ndarray, action: int, lab_items: List) -> np.ndarray:\n        \"\"\"\n        Apply treatment effects to lab values\n        \n        Simulates realistic pharmacological responses\n        \"\"\"\n        \n        labs = labs.copy()\n        \n        # Treatment effect parameters\n        if action == 1:  # Medication A (e.g., insulin)\n            target_items = [50809]  # Glucose\n            effect_strength = 0.15\n            variability = 0.05\n        elif action == 2:  # Medication B (e.g., diuretic)\n            target_items = [50971, 50983]  # Potassium, Sodium\n            effect_strength = 0.10\n            variability = 0.08\n        else:  # action == 3 - Broad treatment\n            target_items = lab_items\n            effect_strength = 0.05\n            variability = 0.10\n        \n        # Apply effects\n        for i, itemid in enumerate(lab_items):\n            if i < len(labs) and itemid in target_items:\n                if itemid in self.lab_item_info:\n                    # Target = center of normal range\n                    normal_range = self.lab_item_info[itemid]['normal_range']\n                    target = (normal_range[0] + normal_range[1]) / 2\n                    \n                    # Move toward target with noise\n                    delta = effect_strength * (target - labs[i])\n                    noise = np.random.normal(0, variability * abs(labs[i] + 1e-6))\n                    labs[i] += delta + noise\n                    \n                    # Ensure positive values\n                    labs[i] = max(0.1, labs[i])\n        \n        return labs\n    \n    def _calculate_reward(self, labs, lab_items, action):\n        \"\"\"\n        SIMPLIFIED & CONSISTENT reward function\n        \n        Key principles:\n        1. Clear positive/negative signals\n        2. Scale matches evaluation expectations\n        3. No complex shaping that breaks transfer\n        \"\"\"\n        \n        abnormal_count = 0\n        normal_count = 0\n        improvement_bonus = 0.0\n        \n        for i, itemid in enumerate(lab_items):\n            if i < len(labs) and itemid in self.lab_item_info:\n                normal_range = self.lab_item_info[itemid]['normal_range']\n                val = labs[i]\n                \n                # Count normal vs abnormal\n                if normal_range[0] <= val <= normal_range[1]:\n                    normal_count += 1\n                else:\n                    abnormal_count += 1\n                    \n                    # IMPROVEMENT TRACKING\n                    if len(self.state_history) > 0 and i < len(self.state_history[-1]):\n                        prev_val = self.state_history[-1][i]\n                        \n                        # Was it abnormal before?\n                        was_abnormal = (prev_val < normal_range[0] or \n                                      prev_val > normal_range[1])\n                        \n                        if was_abnormal:\n                            # Calculate improvement\n                            prev_dist = min(abs(prev_val - normal_range[0]), \n                                          abs(prev_val - normal_range[1]))\n                            curr_dist = min(abs(val - normal_range[0]), \n                                          abs(val - normal_range[1]))\n                            \n                            if curr_dist < prev_dist:\n                                improvement_bonus += 2.0  # Reward improvement\n        \n        # SIMPLE, CLEAR REWARD COMPONENTS\n        # Base reward: Strong positive for normal, moderate negative for abnormal\n        base_reward = (normal_count * 3.0) - (abnormal_count * 1.0)\n        \n        # Treatment cost: Discourage unnecessary treatment\n        if action > 0:\n            # Only penalize if patient is mostly healthy\n            if abnormal_count <= 2:\n                treatment_cost = -2.0  # Don't treat healthy patients!\n            else:\n                treatment_cost = -0.3  # Small cost for necessary treatment\n        else:\n            # Reward for not treating when appropriate\n            treatment_cost = 0.5 if abnormal_count <= 2 else 0.0\n        \n        # Bonus for improvements\n        reward_bonus = improvement_bonus\n        \n        # TOTAL REWARD\n        total_reward = base_reward + treatment_cost + reward_bonus\n        \n        return float(total_reward)\n\n        \n    def _count_abnormal(self, labs: np.ndarray, lab_items: List) -> int:\n        \"\"\"Count number of abnormal lab values\"\"\"\n        \n        count = 0\n        for i, itemid in enumerate(lab_items):\n            if i < len(labs) and itemid in self.lab_item_info:\n                normal_range = self.lab_item_info[itemid]['normal_range']\n                if labs[i] < normal_range[0] or labs[i] > normal_range[1]:\n                    count += 1\n        return count\n\nprint(\"‚úÖ Medical Treatment Environment defined\")\nprint(f\"   ‚Ä¢ State space: {26} dimensions\")\nprint(f\"   ‚Ä¢ Action space: {4} discrete actions\")\nprint(f\"   ‚Ä¢ Reward: Multi-component (clinical + stability - cost)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 6: REINFORCEMENT LEARNING TRAINING\n# PURPOSE: Train RL agents with consistent reward structure\n# ==============================================================================\n\nimport os\nimport json\nfrom stable_baselines3 import PPO, A2C, DQN\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.callbacks import EvalCallback, BaseCallback\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üöÄ SECTION 6: REINFORCEMENT LEARNING TRAINING\")\nprint(\"=\"*80)\n\n# -----------------------------------------------------------------------------\n# 6.1: Create Environments\n# -----------------------------------------------------------------------------\n\nprint(\"\\nüì¶ Creating training environments...\")\n\ntrain_env = DummyVecEnv([lambda: MedicalTreatmentEnv(train_episodes, LAB_ITEM_INFO)])\nval_env = DummyVecEnv([lambda: MedicalTreatmentEnv(val_episodes, LAB_ITEM_INFO)])\ntest_env = MedicalTreatmentEnv(test_episodes, LAB_ITEM_INFO)\n\nprint(f\"‚úÖ Environments created:\")\nprint(f\"   ‚Ä¢ Training episodes: {len(train_episodes)}\")\nprint(f\"   ‚Ä¢ Validation episodes: {len(val_episodes)}\")\nprint(f\"   ‚Ä¢ Test episodes: {len(test_episodes)}\")\n\n# -----------------------------------------------------------------------------\n# 6.2: Enhanced Callbacks\n# -----------------------------------------------------------------------------\n\nclass ProgressCallback(BaseCallback):\n    \"\"\"Track training progress with detailed metrics\"\"\"\n    \n    def __init__(self, verbose=0):\n        super().__init__(verbose)\n        self.episode_rewards = []\n        self.episode_lengths = []\n        \n    def _on_step(self) -> bool:\n        if len(self.model.ep_info_buffer) > 0:\n            for info in self.model.ep_info_buffer:\n                self.episode_rewards.append(info['r'])\n                self.episode_lengths.append(info['l'])\n        return True\n\nclass EarlyStoppingCallback(EvalCallback):\n    \"\"\"Stop training if no improvement for patience evaluations\"\"\"\n    \n    def __init__(self, eval_env, patience=5, min_delta=1.0, **kwargs):\n        super().__init__(eval_env, **kwargs)\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_mean = -np.inf\n        self.wait_count = 0\n        \n    def _on_step(self) -> bool:\n        result = super()._on_step()\n        \n        if len(self.evaluations_results) > 0:\n            current_mean = np.mean(self.evaluations_results[-1])\n            \n            if current_mean > self.best_mean + self.min_delta:\n                self.best_mean = current_mean\n                self.wait_count = 0\n                print(f\"‚ú® New best reward: {current_mean:.2f}\")\n            else:\n                self.wait_count += 1\n                \n            if self.wait_count >= self.patience:\n                print(f\"‚èπÔ∏è  Early stopping triggered (no improvement for {self.patience} evals)\")\n                return False\n                \n        return result\n\n# -----------------------------------------------------------------------------\n# 6.3: Training Configuration (‚úÖ Fixed argument names)\n# -----------------------------------------------------------------------------\n\nTRAINING_CONFIG = {\n    'PPO': {\n        'timesteps': 150000,\n        'learning_rate': 3e-4,       # ‚úÖ renamed from 'lr'\n        'gamma': 0.99,\n        'n_steps': 2048,\n        'batch_size': 64,\n        'n_epochs': 10,\n        'gae_lambda': 0.95,\n        'clip_range': 0.2,\n        'ent_coef': 0.01,\n        'vf_coef': 0.5,\n        'max_grad_norm': 0.5\n    },\n    'A2C': {\n        'timesteps': 120000,\n        'learning_rate': 2e-4,       # ‚úÖ renamed from 'lr'\n        'gamma': 0.99,\n        'n_steps': 5,\n        'gae_lambda': 0.95,\n        'ent_coef': 0.01,\n        'vf_coef': 0.5,\n        'max_grad_norm': 0.5\n    },\n    'DQN': {\n        'timesteps': 150000,\n        'learning_rate': 1e-4,       # ‚úÖ renamed from 'lr'\n        'gamma': 0.99,\n        'buffer_size': 100000,\n        'learning_starts': 1000,\n        'batch_size': 64,\n        'tau': 0.005,\n        'train_freq': 4,\n        'gradient_steps': 1,\n        'target_update_interval': 1000,\n        'exploration_fraction': 0.3,\n        'exploration_initial_eps': 1.0,\n        'exploration_final_eps': 0.05\n    }\n}\n\n\n# -----------------------------------------------------------------------------\n# 6.4: Universal Training Function\n# -----------------------------------------------------------------------------\n\ndef train_rl_agent(algo_class, algo_name, train_env, val_env, config, seed=42):\n    \"\"\"\n    Universal training function for any SB3 algorithm\n    \n    Args:\n        algo_class: PPO, A2C, or DQN class\n        algo_name: String identifier\n        train_env: Training environment\n        val_env: Validation environment\n        config: Hyperparameter dictionary\n        seed: Random seed\n    \n    Returns:\n        Trained model, progress callback\n    \"\"\"\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"üéØ TRAINING {algo_name}\")\n    print(f\"{'='*80}\")\n    \n    # Create save directory\n    save_dir = f\"/kaggle/working/models/{algo_name}_best\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Extract hyperparameters\n    timesteps = config.pop('timesteps')\n    \n    # Setup callbacks\n    eval_callback = EarlyStoppingCallback(\n        val_env,\n        eval_freq=5000,\n        n_eval_episodes=10,\n        best_model_save_path=save_dir,\n        log_path=save_dir,\n        patience=5,\n        min_delta=1.0,\n        deterministic=True,\n        render=False,\n        verbose=1\n    )\n    \n    progress_callback = ProgressCallback(verbose=0)\n    \n    # Initialize model\n    print(f\"üìä Hyperparameters: {config}\")\n    \n    model = algo_class(\n        \"MlpPolicy\",\n        train_env,\n        verbose=1,\n        seed=seed,\n        tensorboard_log=f\"/kaggle/working/tb/{algo_name}\",\n        **config\n    )\n    \n    # Train\n    print(f\"üèÉ Training for {timesteps:,} timesteps...\")\n    model.learn(\n        total_timesteps=timesteps,\n        callback=[eval_callback, progress_callback]\n    )\n    \n    # Save final model\n    model.save(f\"/kaggle/working/models/{algo_name}_final.zip\")\n    print(f\"‚úÖ {algo_name} training complete!\")\n    print(f\"   üìÅ Saved to: /kaggle/working/models/{algo_name}_final.zip\")\n    \n    return model, progress_callback\n\n# -----------------------------------------------------------------------------\n# 6.5: Train All Algorithms\n# -----------------------------------------------------------------------------\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ü§ñ TRAINING ALL ALGORITHMS\")\nprint(\"=\"*80)\n\nmodels = {}\ncallbacks = {}\nSEED = int(config.seed)\n\n# Train PPO\nmodels['PPO'], callbacks['PPO'] = train_rl_agent(\n    PPO, 'PPO', \n    train_env, val_env, \n    TRAINING_CONFIG['PPO'].copy(),\n    seed=SEED\n)\n\n# Train A2C\nmodels['A2C'], callbacks['A2C'] = train_rl_agent(\n    A2C, 'A2C',\n    train_env, val_env,\n    TRAINING_CONFIG['A2C'].copy(),\n    seed=SEED\n)\n\n# Train DQN\nmodels['DQN'], callbacks['DQN'] = train_rl_agent(\n    DQN, 'DQN',\n    train_env, val_env,\n    TRAINING_CONFIG['DQN'].copy(),\n    seed=SEED\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ ALL MODELS TRAINED SUCCESSFULLY!\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T14:04:05.794261Z","iopub.execute_input":"2025-10-21T14:04:05.794973Z","iopub.status.idle":"2025-10-21T14:04:37.427944Z","shell.execute_reply.started":"2025-10-21T14:04:05.794948Z","shell.execute_reply":"2025-10-21T14:04:37.427322Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüöÄ SECTION 6: REINFORCEMENT LEARNING TRAINING\n================================================================================\n\nüì¶ Creating training environments...\n‚úÖ Environments created:\n   ‚Ä¢ Training episodes: 84\n   ‚Ä¢ Validation episodes: 18\n   ‚Ä¢ Test episodes: 18\n\n================================================================================\nü§ñ TRAINING ALL ALGORITHMS\n================================================================================\n\n================================================================================\nüéØ TRAINING PPO\n================================================================================\nüìä Hyperparameters: {'learning_rate': 0.0003, 'gamma': 0.99, 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 10, 'gae_lambda': 0.95, 'clip_range': 0.2, 'ent_coef': 0.01, 'vf_coef': 0.5, 'max_grad_norm': 0.5}\nUsing cuda device\nüèÉ Training for 150,000 timesteps...\nLogging to /kaggle/working/tb/PPO/PPO_1\n-----------------------------\n| time/              |      |\n|    fps             | 531  |\n|    iterations      | 1    |\n|    time_elapsed    | 3    |\n|    total_timesteps | 2048 |\n-----------------------------\n-----------------------------------------\n| time/                   |             |\n|    fps                  | 463         |\n|    iterations           | 2           |\n|    time_elapsed         | 8           |\n|    total_timesteps      | 4096        |\n| train/                  |             |\n|    approx_kl            | 0.010874995 |\n|    clip_fraction        | 0.0643      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.38       |\n|    explained_variance   | 0.00279     |\n|    learning_rate        | 0.0003      |\n|    loss                 | 4.72e+03    |\n|    n_updates            | 10          |\n|    policy_gradient_loss | -0.00546    |\n|    value_loss           | 1.3e+04     |\n-----------------------------------------\nEval num_timesteps=5000, episode_reward=160.23 +/- 111.68\nEpisode length: 16.40 +/- 11.57\n-----------------------------------------\n| eval/                   |             |\n|    mean_ep_length       | 16.4        |\n|    mean_reward          | 160         |\n| time/                   |             |\n|    total_timesteps      | 5000        |\n| train/                  |             |\n|    approx_kl            | 0.008480836 |\n|    clip_fraction        | 0.0525      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.38       |\n|    explained_variance   | 0.000898    |\n|    learning_rate        | 0.0003      |\n|    loss                 | 3.02e+03    |\n|    n_updates            | 20          |\n|    policy_gradient_loss | -0.00671    |\n|    value_loss           | 8.8e+03     |\n-----------------------------------------\nNew best mean reward!\n‚ú® New best reward: 160.23\n‚èπÔ∏è  Early stopping triggered (no improvement for 5 evals)\n‚úÖ PPO training complete!\n   üìÅ Saved to: /kaggle/working/models/PPO_final.zip\n\n================================================================================\nüéØ TRAINING A2C\n================================================================================\nüìä Hyperparameters: {'learning_rate': 0.0002, 'gamma': 0.99, 'n_steps': 5, 'gae_lambda': 0.95, 'ent_coef': 0.01, 'vf_coef': 0.5, 'max_grad_norm': 0.5}\nUsing cuda device\nüèÉ Training for 120,000 timesteps...\nLogging to /kaggle/working/tb/A2C/A2C_1\n------------------------------------\n| time/                 |          |\n|    fps                | 417      |\n|    iterations         | 100      |\n|    time_elapsed       | 1        |\n|    total_timesteps    | 500      |\n| train/                |          |\n|    entropy_loss       | -1.38    |\n|    explained_variance | -0.0105  |\n|    learning_rate      | 0.0002   |\n|    n_updates          | 99       |\n|    policy_loss        | 23.5     |\n|    value_loss         | 369      |\n------------------------------------\n------------------------------------\n| time/                 |          |\n|    fps                | 415      |\n|    iterations         | 200      |\n|    time_elapsed       | 2        |\n|    total_timesteps    | 1000     |\n| train/                |          |\n|    entropy_loss       | -1.36    |\n|    explained_variance | 0.00144  |\n|    learning_rate      | 0.0002   |\n|    n_updates          | 199      |\n|    policy_loss        | 42.4     |\n|    value_loss         | 1.46e+03 |\n------------------------------------\n------------------------------------\n| time/                 |          |\n|    fps                | 410      |\n|    iterations         | 300      |\n|    time_elapsed       | 3        |\n|    total_timesteps    | 1500     |\n| train/                |          |\n|    entropy_loss       | -1.36    |\n|    explained_variance | -0.0181  |\n|    learning_rate      | 0.0002   |\n|    n_updates          | 299      |\n|    policy_loss        | 26.8     |\n|    value_loss         | 411      |\n------------------------------------\n-------------------------------------\n| time/                 |           |\n|    fps                | 409       |\n|    iterations         | 400       |\n|    time_elapsed       | 4         |\n|    total_timesteps    | 2000      |\n| train/                |           |\n|    entropy_loss       | -1.35     |\n|    explained_variance | -0.000233 |\n|    learning_rate      | 0.0002    |\n|    n_updates          | 399       |\n|    policy_loss        | 54        |\n|    value_loss         | 1.58e+03  |\n-------------------------------------\n-------------------------------------\n| time/                 |           |\n|    fps                | 411       |\n|    iterations         | 500       |\n|    time_elapsed       | 6         |\n|    total_timesteps    | 2500      |\n| train/                |           |\n|    entropy_loss       | -1.36     |\n|    explained_variance | -0.000148 |\n|    learning_rate      | 0.0002    |\n|    n_updates          | 499       |\n|    policy_loss        | 50.1      |\n|    value_loss         | 1.44e+03  |\n-------------------------------------\n------------------------------------\n| time/                 |          |\n|    fps                | 412      |\n|    iterations         | 600      |\n|    time_elapsed       | 7        |\n|    total_timesteps    | 3000     |\n| train/                |          |\n|    entropy_loss       | -1.37    |\n|    explained_variance | 0.0029   |\n|    learning_rate      | 0.0002   |\n|    n_updates          | 599      |\n|    policy_loss        | 36.8     |\n|    value_loss         | 1.4e+03  |\n------------------------------------\n------------------------------------\n| time/                 |          |\n|    fps                | 413      |\n|    iterations         | 700      |\n|    time_elapsed       | 8        |\n|    total_timesteps    | 3500     |\n| train/                |          |\n|    entropy_loss       | -1.36    |\n|    explained_variance | 0.000105 |\n|    learning_rate      | 0.0002   |\n|    n_updates          | 699      |\n|    policy_loss        | 35.7     |\n|    value_loss         | 1.02e+03 |\n------------------------------------\n-------------------------------------\n| time/                 |           |\n|    fps                | 410       |\n|    iterations         | 800       |\n|    time_elapsed       | 9         |\n|    total_timesteps    | 4000      |\n| train/                |           |\n|    entropy_loss       | -1.29     |\n|    explained_variance | -9.18e-06 |\n|    learning_rate      | 0.0002    |\n|    n_updates          | 799       |\n|    policy_loss        | 32.9      |\n|    value_loss         | 1.49e+03  |\n-------------------------------------\n------------------------------------\n| time/                 |          |\n|    fps                | 410      |\n|    iterations         | 900      |\n|    time_elapsed       | 10       |\n|    total_timesteps    | 4500     |\n| train/                |          |\n|    entropy_loss       | -1.27    |\n|    explained_variance | 5.19e-06 |\n|    learning_rate      | 0.0002   |\n|    n_updates          | 899      |\n|    policy_loss        | 44.5     |\n|    value_loss         | 1.51e+03 |\n------------------------------------\nEval num_timesteps=5000, episode_reward=176.91 +/- 131.81\nEpisode length: 16.40 +/- 11.57\n------------------------------------\n| eval/                 |          |\n|    mean_ep_length     | 16.4     |\n|    mean_reward        | 177      |\n| time/                 |          |\n|    total_timesteps    | 5000     |\n| train/                |          |\n|    entropy_loss       | -1.21    |\n|    explained_variance | 1.19e-06 |\n|    learning_rate      | 0.0002   |\n|    n_updates          | 999      |\n|    policy_loss        | 42.7     |\n|    value_loss         | 2.38e+03 |\n------------------------------------\nNew best mean reward!\n‚ú® New best reward: 176.91\n-----------------------------\n| time/              |      |\n|    fps             | 404  |\n|    iterations      | 1000 |\n|    time_elapsed    | 12   |\n|    total_timesteps | 5000 |\n-----------------------------\n‚èπÔ∏è  Early stopping triggered (no improvement for 5 evals)\n‚úÖ A2C training complete!\n   üìÅ Saved to: /kaggle/working/models/A2C_final.zip\n\n================================================================================\nüéØ TRAINING DQN\n================================================================================\nüìä Hyperparameters: {'learning_rate': 0.0001, 'gamma': 0.99, 'buffer_size': 100000, 'learning_starts': 1000, 'batch_size': 64, 'tau': 0.005, 'train_freq': 4, 'gradient_steps': 1, 'target_update_interval': 1000, 'exploration_fraction': 0.3, 'exploration_initial_eps': 1.0, 'exploration_final_eps': 0.05}\nUsing cuda device\nüèÉ Training for 150,000 timesteps...\nLogging to /kaggle/working/tb/DQN/DQN_1\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.999    |\n| time/               |          |\n|    episodes         | 4        |\n|    fps              | 2177     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 24       |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.999    |\n| time/               |          |\n|    episodes         | 8        |\n|    fps              | 2415     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 52       |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.994    |\n| time/               |          |\n|    episodes         | 12       |\n|    fps              | 2970     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 261      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.99     |\n| time/               |          |\n|    episodes         | 16       |\n|    fps              | 3007     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 479      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.986    |\n| time/               |          |\n|    episodes         | 20       |\n|    fps              | 2931     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 643      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.983    |\n| time/               |          |\n|    episodes         | 24       |\n|    fps              | 2923     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 791      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.983    |\n| time/               |          |\n|    episodes         | 28       |\n|    fps              | 2914     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 825      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.982    |\n| time/               |          |\n|    episodes         | 32       |\n|    fps              | 2906     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 861      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.981    |\n| time/               |          |\n|    episodes         | 36       |\n|    fps              | 2899     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 894      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.977    |\n| time/               |          |\n|    episodes         | 40       |\n|    fps              | 2336     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 1099     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 10.2     |\n|    n_updates        | 24       |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.973    |\n| time/               |          |\n|    episodes         | 44       |\n|    fps              | 1989     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 1267     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 10.3     |\n|    n_updates        | 66       |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.971    |\n| time/               |          |\n|    episodes         | 48       |\n|    fps              | 1869     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 1356     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 9.13     |\n|    n_updates        | 88       |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.97     |\n| time/               |          |\n|    episodes         | 52       |\n|    fps              | 1814     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 1401     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 8.04     |\n|    n_updates        | 100      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.969    |\n| time/               |          |\n|    episodes         | 56       |\n|    fps              | 1735     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 1477     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 7.63     |\n|    n_updates        | 119      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.968    |\n| time/               |          |\n|    episodes         | 60       |\n|    fps              | 1695     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 1527     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 6.98     |\n|    n_updates        | 131      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.967    |\n| time/               |          |\n|    episodes         | 64       |\n|    fps              | 1666     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 1567     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 8.92     |\n|    n_updates        | 141      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.966    |\n| time/               |          |\n|    episodes         | 68       |\n|    fps              | 1642     |\n|    time_elapsed     | 0        |\n|    total_timesteps  | 1599     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 7.72     |\n|    n_updates        | 149      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.965    |\n| time/               |          |\n|    episodes         | 72       |\n|    fps              | 1602     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 1663     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 6.63     |\n|    n_updates        | 165      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.964    |\n| time/               |          |\n|    episodes         | 76       |\n|    fps              | 1582     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 1697     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 6.74     |\n|    n_updates        | 174      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.961    |\n| time/               |          |\n|    episodes         | 80       |\n|    fps              | 1501     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 1871     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 5.95     |\n|    n_updates        | 217      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.96     |\n| time/               |          |\n|    episodes         | 84       |\n|    fps              | 1484     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 1909     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.79     |\n|    n_updates        | 227      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.957    |\n| time/               |          |\n|    episodes         | 88       |\n|    fps              | 1438     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 2053     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 5.44     |\n|    n_updates        | 263      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.956    |\n| time/               |          |\n|    episodes         | 92       |\n|    fps              | 1425     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 2085     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.99     |\n|    n_updates        | 271      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.955    |\n| time/               |          |\n|    episodes         | 96       |\n|    fps              | 1411     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 2131     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.96     |\n|    n_updates        | 282      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.954    |\n| time/               |          |\n|    episodes         | 100      |\n|    fps              | 1395     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 2181     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.87     |\n|    n_updates        | 295      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.953    |\n| time/               |          |\n|    episodes         | 104      |\n|    fps              | 1380     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 2226     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.2      |\n|    n_updates        | 306      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.951    |\n| time/               |          |\n|    episodes         | 108      |\n|    fps              | 1358     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 2313     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.33     |\n|    n_updates        | 328      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.948    |\n| time/               |          |\n|    episodes         | 112      |\n|    fps              | 1332     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 2450     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.05     |\n|    n_updates        | 362      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.946    |\n| time/               |          |\n|    episodes         | 116      |\n|    fps              | 1315     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 2560     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.42     |\n|    n_updates        | 389      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.945    |\n| time/               |          |\n|    episodes         | 120      |\n|    fps              | 1308     |\n|    time_elapsed     | 1        |\n|    total_timesteps  | 2596     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.43     |\n|    n_updates        | 398      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.941    |\n| time/               |          |\n|    episodes         | 124      |\n|    fps              | 1279     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 2795     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4        |\n|    n_updates        | 448      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.94     |\n| time/               |          |\n|    episodes         | 128      |\n|    fps              | 1273     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 2842     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.01     |\n|    n_updates        | 460      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.939    |\n| time/               |          |\n|    episodes         | 132      |\n|    fps              | 1266     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 2896     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.36     |\n|    n_updates        | 473      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.937    |\n| time/               |          |\n|    episodes         | 136      |\n|    fps              | 1256     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 2962     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.94     |\n|    n_updates        | 490      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.936    |\n| time/               |          |\n|    episodes         | 140      |\n|    fps              | 1250     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 3011     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.87     |\n|    n_updates        | 502      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.936    |\n| time/               |          |\n|    episodes         | 144      |\n|    fps              | 1244     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 3050     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.64     |\n|    n_updates        | 512      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.935    |\n| time/               |          |\n|    episodes         | 148      |\n|    fps              | 1240     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 3093     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.34     |\n|    n_updates        | 523      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.933    |\n| time/               |          |\n|    episodes         | 152      |\n|    fps              | 1230     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 3183     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.94     |\n|    n_updates        | 545      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.932    |\n| time/               |          |\n|    episodes         | 156      |\n|    fps              | 1225     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 3222     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.21     |\n|    n_updates        | 555      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.928    |\n| time/               |          |\n|    episodes         | 160      |\n|    fps              | 1212     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 3387     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.6      |\n|    n_updates        | 596      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.927    |\n| time/               |          |\n|    episodes         | 164      |\n|    fps              | 1207     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 3447     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.96     |\n|    n_updates        | 611      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.926    |\n| time/               |          |\n|    episodes         | 168      |\n|    fps              | 1201     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 3506     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.87     |\n|    n_updates        | 626      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.925    |\n| time/               |          |\n|    episodes         | 172      |\n|    fps              | 1198     |\n|    time_elapsed     | 2        |\n|    total_timesteps  | 3555     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 2.94     |\n|    n_updates        | 638      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.924    |\n| time/               |          |\n|    episodes         | 176      |\n|    fps              | 1193     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 3590     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.87     |\n|    n_updates        | 647      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.923    |\n| time/               |          |\n|    episodes         | 180      |\n|    fps              | 1190     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 3626     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.6      |\n|    n_updates        | 656      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.922    |\n| time/               |          |\n|    episodes         | 184      |\n|    fps              | 1185     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 3683     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.01     |\n|    n_updates        | 670      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.921    |\n| time/               |          |\n|    episodes         | 188      |\n|    fps              | 1181     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 3729     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.58     |\n|    n_updates        | 682      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.916    |\n| time/               |          |\n|    episodes         | 192      |\n|    fps              | 1168     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 3986     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.57     |\n|    n_updates        | 746      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.915    |\n| time/               |          |\n|    episodes         | 196      |\n|    fps              | 1167     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4010     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.15     |\n|    n_updates        | 752      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.914    |\n| time/               |          |\n|    episodes         | 200      |\n|    fps              | 1163     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4077     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.94     |\n|    n_updates        | 769      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.913    |\n| time/               |          |\n|    episodes         | 204      |\n|    fps              | 1159     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4129     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 2.99     |\n|    n_updates        | 782      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.912    |\n| time/               |          |\n|    episodes         | 208      |\n|    fps              | 1155     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4185     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.57     |\n|    n_updates        | 796      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.911    |\n| time/               |          |\n|    episodes         | 212      |\n|    fps              | 1152     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4223     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.74     |\n|    n_updates        | 805      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.91     |\n| time/               |          |\n|    episodes         | 216      |\n|    fps              | 1150     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4266     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.54     |\n|    n_updates        | 816      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.909    |\n| time/               |          |\n|    episodes         | 220      |\n|    fps              | 1147     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4323     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.56     |\n|    n_updates        | 830      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.908    |\n| time/               |          |\n|    episodes         | 224      |\n|    fps              | 1146     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4347     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.18     |\n|    n_updates        | 836      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.907    |\n| time/               |          |\n|    episodes         | 228      |\n|    fps              | 1144     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4384     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.46     |\n|    n_updates        | 845      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.907    |\n| time/               |          |\n|    episodes         | 232      |\n|    fps              | 1142     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4402     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.6      |\n|    n_updates        | 850      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.906    |\n| time/               |          |\n|    episodes         | 236      |\n|    fps              | 1140     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4447     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.74     |\n|    n_updates        | 861      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.905    |\n| time/               |          |\n|    episodes         | 240      |\n|    fps              | 1137     |\n|    time_elapsed     | 3        |\n|    total_timesteps  | 4485     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.55     |\n|    n_updates        | 871      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.904    |\n| time/               |          |\n|    episodes         | 244      |\n|    fps              | 1135     |\n|    time_elapsed     | 4        |\n|    total_timesteps  | 4548     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.73     |\n|    n_updates        | 886      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.903    |\n| time/               |          |\n|    episodes         | 248      |\n|    fps              | 1133     |\n|    time_elapsed     | 4        |\n|    total_timesteps  | 4594     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.67     |\n|    n_updates        | 898      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.902    |\n| time/               |          |\n|    episodes         | 252      |\n|    fps              | 1132     |\n|    time_elapsed     | 4        |\n|    total_timesteps  | 4620     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 2.77     |\n|    n_updates        | 904      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.902    |\n| time/               |          |\n|    episodes         | 256      |\n|    fps              | 1130     |\n|    time_elapsed     | 4        |\n|    total_timesteps  | 4655     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.72     |\n|    n_updates        | 913      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.901    |\n| time/               |          |\n|    episodes         | 260      |\n|    fps              | 1127     |\n|    time_elapsed     | 4        |\n|    total_timesteps  | 4713     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.58     |\n|    n_updates        | 928      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.9      |\n| time/               |          |\n|    episodes         | 264      |\n|    fps              | 1125     |\n|    time_elapsed     | 4        |\n|    total_timesteps  | 4755     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.81     |\n|    n_updates        | 938      |\n----------------------------------\n----------------------------------\n| rollout/            |          |\n|    exploration_rate | 0.898    |\n| time/               |          |\n|    episodes         | 268      |\n|    fps              | 1123     |\n|    time_elapsed     | 4        |\n|    total_timesteps  | 4816     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 4.11     |\n|    n_updates        | 953      |\n----------------------------------\nEval num_timesteps=5000, episode_reward=168.61 +/- 116.43\nEpisode length: 16.40 +/- 11.57\n----------------------------------\n| eval/               |          |\n|    mean_ep_length   | 16.4     |\n|    mean_reward      | 169      |\n| rollout/            |          |\n|    exploration_rate | 0.894    |\n| time/               |          |\n|    total_timesteps  | 5000     |\n| train/              |          |\n|    learning_rate    | 0.0001   |\n|    loss             | 3.32     |\n|    n_updates        | 999      |\n----------------------------------\nNew best mean reward!\n‚ú® New best reward: 168.61\n‚èπÔ∏è  Early stopping triggered (no improvement for 5 evals)\n‚úÖ DQN training complete!\n   üìÅ Saved to: /kaggle/working/models/DQN_final.zip\n\n================================================================================\n‚úÖ ALL MODELS TRAINED SUCCESSFULLY!\n================================================================================\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 7: COMPREHENSIVE MODEL EVALUATION\n# PURPOSE: Evaluate all trained models on test set with clinical metrics\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä SECTION 7: MODEL EVALUATION & COMPARISON\")\nprint(\"=\"*80)\n\n# -----------------------------------------------------------------------------\n# 7.1: Evaluation Functions\n# -----------------------------------------------------------------------------\n\ndef evaluate_model_detailed(model, env, n_episodes, model_name):\n    \"\"\"\n    Comprehensive model evaluation with clinical metrics\n    \n    Returns:\n        Dictionary with rewards, abnormal counts, actions, appropriateness\n    \"\"\"\n    \n    all_rewards = []\n    all_episode_rewards = []\n    all_actions = []\n    all_abnormal_counts = []\n    all_episode_lengths = []\n    \n    for ep_idx in range(n_episodes):\n        obs = env.reset()\n        done = False\n        episode_reward = 0\n        episode_actions = []\n        episode_abnormal = []\n        steps = 0\n        \n        while not done:\n            action, _ = model.predict(obs, deterministic=True)\n            obs, reward, done, info = env.step(action)\n            \n            all_rewards.append(reward)\n            episode_reward += reward\n            episode_actions.append(int(action))\n            episode_abnormal.append(info.get('abnormal_count', 0))\n            steps += 1\n        \n        all_episode_rewards.append(episode_reward)\n        all_actions.extend(episode_actions)\n        all_abnormal_counts.extend(episode_abnormal)\n        all_episode_lengths.append(steps)\n    \n    # Calculate appropriateness\n    appropriate_actions = 0\n    total_actions = len(all_actions)\n    \n    for action, abnormal in zip(all_actions, all_abnormal_counts):\n        # Appropriate if: treating when sick (‚â•3) OR not treating when healthy (<2)\n        if (action > 0 and abnormal >= 3) or (action == 0 and abnormal < 2):\n            appropriate_actions += 1\n    \n    appropriateness = appropriate_actions / total_actions if total_actions > 0 else 0\n    \n    return {\n        'model_name': model_name,\n        'mean_reward': np.mean(all_episode_rewards),\n        'std_reward': np.std(all_episode_rewards),\n        'mean_step_reward': np.mean(all_rewards),\n        'mean_abnormal': np.mean(all_abnormal_counts),\n        'std_abnormal': np.std(all_abnormal_counts),\n        'treatment_rate': np.mean(np.array(all_actions) > 0),\n        'action_distribution': np.bincount(all_actions, minlength=4),\n        'appropriateness': appropriateness,\n        'mean_episode_length': np.mean(all_episode_lengths),\n        'all_episode_rewards': all_episode_rewards,\n        'all_actions': all_actions,\n        'all_abnormal_counts': all_abnormal_counts\n    }\n\ndef evaluate_baseline(env, n_episodes, policy_name, policy_fn):\n    \"\"\"Evaluate a baseline policy\"\"\"\n    \n    episode_rewards = []\n    \n    for _ in range(n_episodes):\n        obs = env.reset()\n        done = False\n        episode_reward = 0\n        step = 0\n        \n        while not done:\n            action = policy_fn(obs, step)\n            obs, reward, done, info = env.step(action)\n            episode_reward += reward\n            step += 1\n        \n        episode_rewards.append(episode_reward)\n    \n    return {\n        'policy_name': policy_name,\n        'mean_reward': np.mean(episode_rewards),\n        'std_reward': np.std(episode_rewards)\n    }\n\n# -----------------------------------------------------------------------------\n# 7.2: Evaluate Trained Models\n# -----------------------------------------------------------------------------\n\nprint(\"\\nüìà Evaluating trained models on test set...\")\n\nN_TEST_EPISODES = min(50, len(test_episodes))\nresults = {}\n\nfor name, model in models.items():\n    print(f\"\\nüîç Evaluating {name}...\")\n    results[name] = evaluate_model_detailed(\n        model, test_env, N_TEST_EPISODES, name\n    )\n    \n    print(f\"   Mean Episode Reward: {results[name]['mean_reward']:.2f} ¬± {results[name]['std_reward']:.2f}\")\n    print(f\"   Mean Abnormal Labs: {results[name]['mean_abnormal']:.2f}\")\n    print(f\"   Treatment Rate: {results[name]['treatment_rate']:.1%}\")\n    print(f\"   Clinical Appropriateness: {results[name]['appropriateness']:.1%}\")\n\n# -----------------------------------------------------------------------------\n# 7.3: Evaluate Baselines\n# -----------------------------------------------------------------------------\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä BASELINE COMPARISON\")\nprint(\"=\"*80)\n\nbaselines = {}\n\n# Random policy\nbaselines['Random'] = evaluate_baseline(\n    test_env, N_TEST_EPISODES, 'Random',\n    lambda obs, step: test_env.action_space.sample()\n)\n\n# Always treat (action 1)\nbaselines['Always_Treat'] = evaluate_baseline(\n    test_env, N_TEST_EPISODES, 'Always Treat',\n    lambda obs, step: 1\n)\n\n# Never treat (action 0)\nbaselines['Never_Treat'] = evaluate_baseline(\n    test_env, N_TEST_EPISODES, 'Never Treat',\n    lambda obs, step: 0\n)\n\n# Smart baseline: treat if many abnormal (estimated from normalized values)\ndef smart_policy(obs, step):\n    normalized_vals = obs[10:20]  # Normalized lab values\n    abnormal_est = np.sum(np.abs(normalized_vals) > 1.0)\n    return 1 if abnormal_est >= 3 else 0\n\nbaselines['Smart_Baseline'] = evaluate_baseline(\n    test_env, N_TEST_EPISODES, 'Smart Baseline',\n    smart_policy\n)\n\nprint(\"\\nBaseline Results:\")\nfor name, result in baselines.items():\n    print(f\"  {name:20s}: {result['mean_reward']:7.2f} ¬± {result['std_reward']:.2f}\")\n\nprint(\"\\nTrained Models:\")\nfor name in models.keys():\n    print(f\"  {name:20s}: {results[name]['mean_reward']:7.2f} ¬± {results[name]['std_reward']:.2f}\")\n\n# -----------------------------------------------------------------------------\n# 7.4: Find Best Model\n# -----------------------------------------------------------------------------\n\nbest_model_name = max(results.keys(), key=lambda k: results[k]['mean_reward'])\nbest_model = models[best_model_name]\n\nprint(\"\\n\" + \"=\"*80)\nprint(f\"üèÜ BEST MODEL: {best_model_name}\")\nprint(\"=\"*80)\nprint(f\"Mean Episode Reward: {results[best_model_name]['mean_reward']:.2f}\")\nprint(f\"Mean Abnormal Labs: {results[best_model_name]['mean_abnormal']:.2f}\")\nprint(f\"Treatment Rate: {results[best_model_name]['treatment_rate']:.1%}\")\nprint(f\"Clinical Appropriateness: {results[best_model_name]['appropriateness']:.1%}\")\n\n# Calculate improvements\nbest_reward = results[best_model_name]['mean_reward']\nprint(f\"\\nImprovement over baselines:\")\nprint(f\"  vs Random: {best_reward - baselines['Random']['mean_reward']:+.2f}\")\nprint(f\"  vs Always Treat: {best_reward - baselines['Always_Treat']['mean_reward']:+.2f}\")\nprint(f\"  vs Never Treat: {best_reward - baselines['Never_Treat']['mean_reward']:+.2f}\")\nprint(f\"  vs Smart Baseline: {best_reward - baselines['Smart_Baseline']['mean_reward']:+.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T14:04:37.429104Z","iopub.execute_input":"2025-10-21T14:04:37.429916Z","iopub.status.idle":"2025-10-21T14:04:38.547920Z","shell.execute_reply.started":"2025-10-21T14:04:37.429896Z","shell.execute_reply":"2025-10-21T14:04:38.547343Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüìä SECTION 7: MODEL EVALUATION & COMPARISON\n================================================================================\n\nüìà Evaluating trained models on test set...\n\nüîç Evaluating PPO...\n   Mean Episode Reward: 128.46 ¬± 150.87\n   Mean Abnormal Labs: 5.83\n   Treatment Rate: 63.2%\n   Clinical Appropriateness: 62.6%\n\nüîç Evaluating A2C...\n   Mean Episode Reward: 244.44 ¬± 193.77\n   Mean Abnormal Labs: 5.57\n   Treatment Rate: 100.0%\n   Clinical Appropriateness: 99.5%\n\nüîç Evaluating DQN...\n   Mean Episode Reward: 133.09 ¬± 105.03\n   Mean Abnormal Labs: 5.25\n   Treatment Rate: 51.2%\n   Clinical Appropriateness: 51.2%\n\n================================================================================\nüìä BASELINE COMPARISON\n================================================================================\n\nBaseline Results:\n  Random              :  146.68 ¬± 135.91\n  Always_Treat        :   99.77 ¬± 136.19\n  Never_Treat         :  114.69 ¬± 127.58\n  Smart_Baseline      :  168.23 ¬± 152.59\n\nTrained Models:\n  PPO                 :  128.46 ¬± 150.87\n  A2C                 :  244.44 ¬± 193.77\n  DQN                 :  133.09 ¬± 105.03\n\n================================================================================\nüèÜ BEST MODEL: A2C\n================================================================================\nMean Episode Reward: 244.44\nMean Abnormal Labs: 5.57\nTreatment Rate: 100.0%\nClinical Appropriateness: 99.5%\n\nImprovement over baselines:\n  vs Random: +97.77\n  vs Always Treat: +144.68\n  vs Never Treat: +129.75\n  vs Smart Baseline: +76.22\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 8: CLINICAL FILTERING & SAFETY CONSTRAINTS\n# PURPOSE: Add clinical safety rules to improve appropriateness\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üè• SECTION 8: CLINICAL FILTERING & SAFETY\")\nprint(\"=\"*80)\n\n# -----------------------------------------------------------------------------\n# 8.1: Clinical Filter Implementation\n# -----------------------------------------------------------------------------\n\nclass ClinicalSafetyFilter:\n    \"\"\"\n    Wrapper that enforces clinical safety constraints on RL policies\n    \n    Safety Rules:\n    1. Don't treat stable patients (abnormal < 2)\n    2. Always treat sick patients (abnormal ‚â• 3)\n    3. Trust RL for borderline cases (abnormal = 2)\n    \"\"\"\n    \n    def __init__(self, base_model, treat_threshold=3, stable_threshold=2):\n        self.base_model = base_model\n        self.treat_threshold = treat_threshold\n        self.stable_threshold = stable_threshold\n        \n    def predict(self, obs, abnormal_count, deterministic=True):\n        \"\"\"Get clinically-filtered action\"\"\"\n        \n        # Get RL model's suggestion\n        rl_action, _ = self.base_model.predict(obs, deterministic=deterministic)\n        \n        # Apply clinical safety rules\n        if abnormal_count < self.stable_threshold:\n            # Rule 1: Patient is stable ‚Üí No treatment\n            return 0, None\n        \n        elif abnormal_count >= self.treat_threshold:\n            # Rule 2: Patient is sick ‚Üí Ensure treatment\n            if rl_action == 0:\n                return 1, None  # Override to treat\n            else:\n                return rl_action, None  # Trust RL's treatment choice\n        \n        else:\n            # Rule 3: Borderline case ‚Üí Trust RL\n            return rl_action, None\n\n# -----------------------------------------------------------------------------\n# 8.2: Evaluate Raw vs Filtered Policies\n# -----------------------------------------------------------------------------\n\ndef compare_raw_vs_filtered(model, model_name, episodes, env_class, lab_info):\n    \"\"\"\n    Compare performance of raw RL policy vs clinically-filtered policy\n    \"\"\"\n    \n    filtered_policy = ClinicalSafetyFilter(model)\n    \n    results = {\n        'raw': {\n            'episode_rewards': [],\n            'treatment_rates': [],\n            'appropriateness_scores': [],\n            'abnormal_counts': []\n        },\n        'filtered': {\n            'episode_rewards': [],\n            'treatment_rates': [],\n            'appropriateness_scores': [],\n            'abnormal_counts': []\n        }\n    }\n    \n    for episode in episodes:\n        env = env_class([episode], lab_info)\n        \n        # -------------------------\n        # Evaluate RAW policy\n        # -------------------------\n        obs = env.reset()\n        done = False\n        episode_data = {\n            'rewards': [],\n            'actions': [],\n            'abnormal': []\n        }\n        \n        while not done:\n            action, _ = model.predict(obs, deterministic=True)\n            obs, reward, done, info = env.step(action)\n            \n            episode_data['rewards'].append(reward)\n            episode_data['actions'].append(int(action))\n            episode_data['abnormal'].append(info.get('abnormal_count', 0))\n        \n        # Calculate metrics\n        total_reward = sum(episode_data['rewards'])\n        treatment_rate = np.mean(np.array(episode_data['actions']) > 0)\n        \n        appropriate = sum(\n            1 for action, abnormal in zip(episode_data['actions'], episode_data['abnormal'])\n            if (action > 0 and abnormal >= 3) or (action == 0 and abnormal < 2)\n        )\n        appropriateness = appropriate / len(episode_data['actions'])\n        \n        results['raw']['episode_rewards'].append(total_reward)\n        results['raw']['treatment_rates'].append(treatment_rate)\n        results['raw']['appropriateness_scores'].append(appropriateness)\n        results['raw']['abnormal_counts'].extend(episode_data['abnormal'])\n        \n        # -------------------------\n        # Evaluate FILTERED policy\n        # -------------------------\n        obs = env.reset()\n        done = False\n        episode_data = {\n            'rewards': [],\n            'actions': [],\n            'abnormal': []\n        }\n        \n        info = {'abnormal_count': 3}  # Initialize\n        \n        while not done:\n            abnormal_count = info.get('abnormal_count', 3)\n            action, _ = filtered_policy.predict(obs, abnormal_count)\n            obs, reward, done, info = env.step(action)\n            \n            episode_data['rewards'].append(reward)\n            episode_data['actions'].append(int(action))\n            episode_data['abnormal'].append(info.get('abnormal_count', 0))\n        \n        total_reward = sum(episode_data['rewards'])\n        treatment_rate = np.mean(np.array(episode_data['actions']) > 0)\n        \n        appropriate = sum(\n            1 for action, abnormal in zip(episode_data['actions'], episode_data['abnormal'])\n            if (action > 0 and abnormal >= 3) or (action == 0 and abnormal < 2)\n        )\n        appropriateness = appropriate / len(episode_data['actions'])\n        \n        results['filtered']['episode_rewards'].append(total_reward)\n        results['filtered']['treatment_rates'].append(treatment_rate)\n        results['filtered']['appropriateness_scores'].append(appropriateness)\n        results['filtered']['abnormal_counts'].extend(episode_data['abnormal'])\n    \n    # Aggregate results\n    summary = {\n        'raw': {\n            'mean_reward': np.mean(results['raw']['episode_rewards']),\n            'std_reward': np.std(results['raw']['episode_rewards']),\n            'treatment_rate': np.mean(results['raw']['treatment_rates']),\n            'appropriateness': np.mean(results['raw']['appropriateness_scores']),\n            'mean_abnormal': np.mean(results['raw']['abnormal_counts'])\n        },\n        'filtered': {\n            'mean_reward': np.mean(results['filtered']['episode_rewards']),\n            'std_reward': np.std(results['filtered']['episode_rewards']),\n            'treatment_rate': np.mean(results['filtered']['treatment_rates']),\n            'appropriateness': np.mean(results['filtered']['appropriateness_scores']),\n            'mean_abnormal': np.mean(results['filtered']['abnormal_counts'])\n        }\n    }\n    \n    return summary\n\n# -----------------------------------------------------------------------------\n# 8.3: Apply Filtering to All Models\n# -----------------------------------------------------------------------------\n\nprint(\"\\nüî¨ Comparing raw vs filtered policies...\")\n\nfiltering_results = {}\n\nfor name, model in models.items():\n    print(f\"\\n{name}:\")\n    print(\"-\" * 40)\n    \n    result = compare_raw_vs_filtered(\n        model, name, \n        test_episodes, \n        MedicalTreatmentEnv, \n        LAB_ITEM_INFO\n    )\n    \n    filtering_results[name] = result\n    \n    print(f\"RAW POLICY:\")\n    print(f\"  Reward: {result['raw']['mean_reward']:.2f} ¬± {result['raw']['std_reward']:.2f}\")\n    print(f\"  Treatment Rate: {result['raw']['treatment_rate']:.1%}\")\n    print(f\"  Appropriateness: {result['raw']['appropriateness']:.1%}\")\n    \n    print(f\"\\nFILTERED POLICY:\")\n    print(f\"  Reward: {result['filtered']['mean_reward']:.2f} ¬± {result['filtered']['std_reward']:.2f}\")\n    print(f\"  Treatment Rate: {result['filtered']['treatment_rate']:.1%}\")\n    print(f\"  Appropriateness: {result['filtered']['appropriateness']:.1%}\")\n    \n    improvement = (result['filtered']['appropriateness'] - result['raw']['appropriateness']) * 100\n    print(f\"\\n‚ú® IMPROVEMENT: +{improvement:.1f} percentage points\")\n\n# -----------------------------------------------------------------------------\n# 8.4: Summary Table\n# -----------------------------------------------------------------------------\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìã CLINICAL FILTERING SUMMARY\")\nprint(\"=\"*80)\n\nsummary_data = []\nfor name in models.keys():\n    raw = filtering_results[name]['raw']\n    filtered = filtering_results[name]['filtered']\n    improvement = (filtered['appropriateness'] - raw['appropriateness']) * 100\n    \n    summary_data.append({\n        'Model': name,\n        'Raw Appropriateness': f\"{raw['appropriateness']:.1%}\",\n        'Filtered Appropriateness': f\"{filtered['appropriateness']:.1%}\",\n        'Improvement': f\"+{improvement:.1f}%\"\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(summary_df.to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T14:04:38.548560Z","iopub.execute_input":"2025-10-21T14:04:38.548746Z","iopub.status.idle":"2025-10-21T14:04:40.573549Z","shell.execute_reply.started":"2025-10-21T14:04:38.548732Z","shell.execute_reply":"2025-10-21T14:04:40.572714Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüè• SECTION 8: CLINICAL FILTERING & SAFETY\n================================================================================\n\nüî¨ Comparing raw vs filtered policies...\n\nPPO:\n----------------------------------------\nRAW POLICY:\n  Reward: 152.27 ¬± 161.79\n  Treatment Rate: 56.8%\n  Appropriateness: 55.0%\n\nFILTERED POLICY:\n  Reward: 148.94 ¬± 159.47\n  Treatment Rate: 98.4%\n  Appropriateness: 94.5%\n\n‚ú® IMPROVEMENT: +39.5 percentage points\n\nA2C:\n----------------------------------------\nRAW POLICY:\n  Reward: 169.01 ¬± 144.80\n  Treatment Rate: 100.0%\n  Appropriateness: 99.4%\n\nFILTERED POLICY:\n  Reward: 177.02 ¬± 152.61\n  Treatment Rate: 100.0%\n  Appropriateness: 98.8%\n\n‚ú® IMPROVEMENT: +-0.6 percentage points\n\nDQN:\n----------------------------------------\nRAW POLICY:\n  Reward: 176.11 ¬± 145.86\n  Treatment Rate: 55.8%\n  Appropriateness: 54.7%\n\nFILTERED POLICY:\n  Reward: 173.51 ¬± 140.40\n  Treatment Rate: 98.0%\n  Appropriateness: 95.5%\n\n‚ú® IMPROVEMENT: +40.8 percentage points\n\n================================================================================\nüìã CLINICAL FILTERING SUMMARY\n================================================================================\nModel Raw Appropriateness Filtered Appropriateness Improvement\n  PPO               55.0%                    94.5%      +39.5%\n  A2C               99.4%                    98.8%      +-0.6%\n  DQN               54.7%                    95.5%      +40.8%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 9: DETAILED MODEL ANALYSIS & REPORTING\n# PURPOSE: Visualize best model behavior and export final results\n# ==============================================================================\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime  # ‚úÖ Fix for NameError\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìà SECTION 9: DETAILED MODEL ANALYSIS & REPORTING\")\nprint(\"=\"*80)\n\n# -----------------------------------------------------------------------------\n# 9.1: Visualization for Best Model\n# -----------------------------------------------------------------------------\n\nprint(f\"\\nüîç Creating detailed analysis for {best_model_name}...\")\n\nbm = results[best_model_name]\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle(f\"Detailed Analysis ‚Äî {best_model_name}\", fontsize=16, fontweight='bold')\n\n# --- Subplot 1: Reward Distribution ---\nax = axes[0, 0]\nreward_data = bm.get(\"rewards\") or bm.get(\"all_episode_rewards\") or [bm[\"mean_reward\"]]\nax.hist(np.array(reward_data).flatten(), bins=20, color='#45B7D1', edgecolor='black', alpha=0.8)\nax.set_title('Reward Distribution', fontsize=13, fontweight='bold')\nax.set_xlabel('Episode Reward')\nax.set_ylabel('Frequency')\nax.grid(alpha=0.3, linestyle='--')\n\n# --- Subplot 2: Abnormal Labs Over Time ---\nax = axes[0, 1]\nepisode_steps = list(range(len(bm.get(\"all_abnormal_counts\", []))))\nax.plot(episode_steps, bm.get(\"all_abnormal_counts\", []), color='#FF6B6B', linewidth=2)\nax.set_title('Abnormal Labs Over Time', fontsize=13, fontweight='bold')\nax.set_xlabel('Episode Step')\nax.set_ylabel('Abnormal Lab Count')\nax.grid(alpha=0.3, linestyle='--')\n\n# --- Subplot 3: Action Distribution ---\nax = axes[1, 0]\nactions = bm.get('all_actions', [])\nif len(actions) > 0:\n    sns.countplot(x=actions, palette='viridis', ax=ax)\n    ax.set_title('Action Distribution', fontsize=13, fontweight='bold')\n    ax.set_xlabel('Action')\n    ax.set_ylabel('Frequency')\nelse:\n    ax.text(0.5, 0.5, 'No Actions Recorded', ha='center', va='center')\n\n# --- Subplot 4: Appropriateness Summary ---\nax = axes[1, 1]\nbar_data = {\n    'Metric': ['Appropriateness', 'Treatment Rate', 'Mean Abnormal'],\n    'Value': [\n        bm.get('appropriateness', 0) * 100,\n        bm.get('treatment_rate', 0) * 100,\n        bm.get('mean_abnormal', 0)\n    ]\n}\nsns.barplot(x='Metric', y='Value', data=pd.DataFrame(bar_data), ax=ax, palette='coolwarm')\nax.set_title('Clinical Metrics Summary', fontsize=13, fontweight='bold')\nax.set_ylabel('Value (%)')\nax.grid(alpha=0.3, linestyle='--')\n\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n# -----------------------------------------------------------------------------\n# 9.2: Generate Final JSON Report (Safe Serialization)\n# -----------------------------------------------------------------------------\n\nprint(\"\\nüìù Generating final JSON report...\")\n\ndef safe_convert(obj):\n    \"\"\"Recursively convert NumPy data to JSON-safe Python-native types.\"\"\"\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()\n    if isinstance(obj, (np.float32, np.float64)):\n        return float(obj)\n    if isinstance(obj, (np.int32, np.int64)):\n        return int(obj)\n    if isinstance(obj, dict):\n        return {k: safe_convert(v) for k, v in obj.items()}\n    if isinstance(obj, list):\n        return [safe_convert(v) for v in obj]\n    return obj\n\nfinal_report = {\n    \"best_model\": best_model_name,\n    \"timestamp\": str(datetime.now()),\n    \"summary\": safe_convert({\n        \"mean_reward\": bm[\"mean_reward\"],\n        \"std_reward\": bm[\"std_reward\"],\n        \"treatment_rate\": bm[\"treatment_rate\"],\n        \"appropriateness\": bm[\"appropriateness\"],\n        \"mean_abnormal\": bm[\"mean_abnormal\"],\n    }),\n    \"filtering_results\": safe_convert(filtering_results.get(best_model_name, {})),\n    \"baselines\": safe_convert({k: v[\"mean_reward\"] for k, v in baselines.items()}),\n}\n\noutput_path = \"/kaggle/working/final_report.json\"\nwith open(output_path, \"w\") as f:\n    json.dump(safe_convert(final_report), f, indent=4)\n\nprint(f\"‚úÖ Final report saved to: {output_path}\")\n\n# ==============================================================================\n# SECTION 9B: COMPREHENSIVE VISUALIZATION & REPORTING\n# PURPOSE: Publication-quality visualizations and comparison plots\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä SECTION 9B: VISUALIZATION & FINAL REPORT\")\nprint(\"=\"*80)\n\n# -----------------------------------------------------------------------------\n# 9B.1: Algorithm Performance Comparison\n# -----------------------------------------------------------------------------\n\nprint(\"\\nüìà Creating performance comparison plots...\")\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\nfig.suptitle('Reinforcement Learning Model Performance Comparison', \n             fontsize=16, fontweight='bold', y=0.995)\n\nmodel_names = list(models.keys())\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n\n# Plot 1: Episode Rewards\nax = axes[0, 0]\nrewards = [results[name]['mean_reward'] for name in model_names]\nerrors = [results[name]['std_reward'] for name in model_names]\nbars = ax.bar(model_names, rewards, yerr=errors, capsize=8, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\nfor i, (bar, val) in enumerate(zip(bars, rewards)):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\nax.set_title('Mean Episode Reward', fontsize=13, fontweight='bold')\nax.set_ylabel('Reward')\nax.grid(axis='y', alpha=0.3, linestyle='--')\nax.set_axisbelow(True)\n\n# Plot 2: Mean Abnormal Labs\nax = axes[0, 1]\nabnormal = [results[name]['mean_abnormal'] for name in model_names]\nbars = ax.bar(model_names, abnormal, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\nfor bar, val in zip(bars, abnormal):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\nax.axhline(y=5, color='red', linestyle='--', linewidth=2, alpha=0.6, label='Target (‚â§5)')\nax.set_title('Mean Abnormal Lab Count', fontsize=13, fontweight='bold')\nax.set_ylabel('Count')\nax.legend()\nax.grid(axis='y', alpha=0.3, linestyle='--')\nax.set_axisbelow(True)\n\n# Plot 3: Clinical Appropriateness (Raw vs Filtered)\nax = axes[1, 0]\nx = np.arange(len(model_names))\nwidth = 0.35\nraw_app = [filtering_results[name]['raw']['appropriateness'] * 100 for name in model_names]\nfiltered_app = [filtering_results[name]['filtered']['appropriateness'] * 100 for name in model_names]\nbars1 = ax.bar(x - width/2, raw_app, width, label='Raw', color='#FFB6B9', edgecolor='black')\nbars2 = ax.bar(x + width/2, filtered_app, width, label='Filtered', color='#A8E6CF', edgecolor='black')\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{bar.get_height():.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\nax.set_title('Clinical Appropriateness: Raw vs Filtered', fontsize=13, fontweight='bold')\nax.set_ylabel('Appropriateness (%)')\nax.set_xticks(x)\nax.set_xticklabels(model_names)\nax.legend()\nax.grid(axis='y', alpha=0.3, linestyle='--')\n\n# Plot 4: Action Distribution\nax = axes[1, 1]\naction_labels = ['No Action', 'Med A', 'Med B', 'Med C']\nx = np.arange(len(action_labels))\nwidth = 0.25\nfor i, name in enumerate(model_names):\n    action_dist = results[name]['action_distribution']\n    total = action_dist.sum() if np.sum(action_dist) != 0 else 1\n    ax.bar(x + i*width, (action_dist / total) * 100, width, label=name, color=colors[i], alpha=0.8, edgecolor='black')\nax.set_title('Action Distribution', fontsize=13, fontweight='bold')\nax.set_ylabel('Percentage (%)')\nax.set_xticks(x + width)\nax.set_xticklabels(action_labels)\nax.legend()\nax.grid(axis='y', alpha=0.3, linestyle='--')\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/model_comparison.png', dpi=300, bbox_inches='tight')\nprint(\"‚úÖ Saved: model_comparison.png\")\nplt.show()\n\n# -----------------------------------------------------------------------------\n# 9B.2: Final Comprehensive Report (JSON Safe)\n# -----------------------------------------------------------------------------\n\nprint(\"\\nüìù Generating comprehensive final report...\")\n\nfinal_report_full = {\n    \"best_model\": best_model_name,\n    \"timestamp\": str(datetime.now()),\n    \"summary\": safe_convert({\n        \"mean_reward\": bm[\"mean_reward\"],\n        \"std_reward\": bm[\"std_reward\"],\n        \"mean_abnormal\": bm[\"mean_abnormal\"],\n        \"treatment_rate\": bm[\"treatment_rate\"],\n        \"appropriateness\": bm[\"appropriateness\"],\n    }),\n    \"comparisons\": safe_convert(results),\n    \"filtering_results\": safe_convert(filtering_results),\n    \"baselines\": safe_convert(baselines),\n}\n\nreport_path = \"/kaggle/working/final_evaluation_report.json\"\nwith open(report_path, \"w\") as f:\n    json.dump(safe_convert(final_report_full), f, indent=4)\n\nprint(f\"‚úÖ Final comprehensive report saved to: {report_path}\")\nprint(\"\\nüéØ Visualization & Reporting Complete! All figures and reports saved in /kaggle/working/\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}